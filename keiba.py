#åŸºæœ¬ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
import os
import streamlit as st
import pandas as pd
import numpy as np
from sklearn.metrics import roc_auc_score
from sklearn.preprocessing import LabelEncoder
import lightgbm as lgb
import requests
from bs4 import BeautifulSoup
import time
import re
from urllib.request import urlopen
import optuna.integration.lightgbm as lgb_o
from itertools import combinations, permutations
import matplotlib.pyplot as plt
from io import StringIO
import datetime



class DataProcessor:
    """
    Attributes:
    ----------
    data : pd.DataFrame
        rawãƒ‡ãƒ¼ã‚¿
    data_p : pd.DataFrame
        preprocessingå¾Œã®ãƒ‡ãƒ¼ã‚¿
    data_h : pd.DataFrame
        merge_horse_resultså¾Œã®ãƒ‡ãƒ¼ã‚¿
    data_pe : pd.DataFrame
        merge_pedså¾Œã®ãƒ‡ãƒ¼ã‚¿
    data_c : pd.DataFrame
        process_categoricalå¾Œã®ãƒ‡ãƒ¼ã‚¿
    no_peds: Numpy.array
        merge_pedsã‚’å®Ÿè¡Œã—ãŸæ™‚ã«ã€è¡€çµ±ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ãªã‹ã£ãŸé¦¬ã®horse_idä¸€è¦§
    """

    def __init__(self):
        self.data = pd.DataFrame()
        self.data_p = pd.DataFrame()
        self.data_h = pd.DataFrame()
        self.data_pe = pd.DataFrame()
        self.data_c = pd.DataFrame()

    def merge_horse_results(self, hr, n_samples_list=[5, 9, 'all']):
        """
        é¦¬ã®éå»æˆç¸¾ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã€
        n_samples_listã§æŒ‡å®šã•ã‚ŒãŸãƒ¬ãƒ¼ã‚¹åˆ†ã®ç€é †ã¨è³é‡‘ã®å¹³å‡ã‚’è¿½åŠ ã—ã¦data_hã«è¿”ã™

        Parameters:
        ----------
        hr : HorseResults
            é¦¬ã®éå»æˆç¸¾ãƒ‡ãƒ¼ã‚¿
        n_samples_list : list, default [5, 9, 'all']
            éå»ä½•ãƒ¬ãƒ¼ã‚¹åˆ†è¿½åŠ ã™ã‚‹ã‹
        """

        self.data_h = self.data_p.copy()
        for n_samples in n_samples_list:
            self.data_h = hr.merge_all(self.data_h, n_samples=n_samples)

        #6/6è¿½åŠ ï¼š é¦¬ã®å‡ºèµ°é–“éš”è¿½åŠ 
        self.data_h['interval'] = (self.data_h['date'] - self.data_h['latest']).dt.days
        self.data_h.drop(['é–‹å‚¬', 'latest'], axis=1, inplace=True)

    def merge_peds(self, peds):
        """
        5ä¸–ä»£åˆ†è¡€çµ±ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ã—ã¦data_peã«è¿”ã™

        Parameters:
        ----------
        peds : Peds.peds_e
            Pedsã‚¯ãƒ©ã‚¹ã§åŠ å·¥ã•ã‚ŒãŸè¡€çµ±ãƒ‡ãƒ¼ã‚¿ã€‚
        """

        self.data_pe = \
            self.data_h.merge(peds, left_on='horse_id', right_index=True,
                                                             how='left')
        self.no_peds = self.data_pe[self.data_pe['peds_0'].isnull()]\
            ['horse_id'].unique()
        if len(self.no_peds) > 0:
            print('scrape peds at horse_id_list "no_peds"')

    def process_categorical(self, le_horse, le_jockey, results_m):
        """
        ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã‚’å‡¦ç†ã—ã¦data_cã«è¿”ã™

        Parameters:
        ----------
        le_horse : sklearn.preprocessing.LabelEncoder
            horse_idã‚’0å§‹ã¾ã‚Šã®æ•´æ•°ã«å¤‰æ›ã™ã‚‹LabelEncoderã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€‚
        le_jockey : sklearn.preprocessing.LabelEncoder
            jockey_idã‚’0å§‹ã¾ã‚Šã®æ•´æ•°ã«å¤‰æ›ã™ã‚‹LabelEncoderã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€‚
        results_m : Results.data_pe
            ãƒ€ãƒŸãƒ¼å¤‰æ•°åŒ–ã®ã¨ãã€Resultsã‚¯ãƒ©ã‚¹ã¨ShutubaTableã‚¯ãƒ©ã‚¹ã§åˆ—ã‚’åˆã‚ã›ã‚‹ãŸã‚ã®ã‚‚ã®
        """

        df = self.data_pe.copy()

        #ãƒ©ãƒ™ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã€‚horse_id, jockey_idã‚’0å§‹ã¾ã‚Šã®æ•´æ•°ã«å¤‰æ›
        mask_horse = df['horse_id'].isin(le_horse.classes_)
        new_horse_id = df['horse_id'].mask(mask_horse).dropna().unique()
        le_horse.classes_ = np.concatenate([le_horse.classes_, new_horse_id])
        df['horse_id'] = le_horse.transform(df['horse_id'])
        mask_jockey = df['jockey_id'].isin(le_jockey.classes_)
        new_jockey_id = df['jockey_id'].mask(mask_jockey).dropna().unique()
        le_jockey.classes_ = np.concatenate([le_jockey.classes_, new_jockey_id])
        df['jockey_id'] = le_jockey.transform(df['jockey_id'])

        #horse_id, jockey_idã‚’pandasã®categoryå‹ã«å¤‰æ›
        df['horse_id'] = df['horse_id'].astype('category')
        df['jockey_id'] = df['jockey_id'].astype('category')

        #ãã®ã»ã‹ã®ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã‚’pandasã®categoryå‹ã«å¤‰æ›ã—ã¦ã‹ã‚‰ãƒ€ãƒŸãƒ¼å¤‰æ•°åŒ–
        #åˆ—ã‚’ä¸€å®šã«ã™ã‚‹ãŸã‚
        weathers = results_m['weather'].unique()
        race_types = results_m['race_type'].unique()
        ground_states = results_m['ground_state'].unique()
        sexes = results_m['æ€§'].unique()
        df['weather'] = pd.Categorical(df['weather'], weathers)
        df['race_type'] = pd.Categorical(df['race_type'], race_types)
        df['ground_state'] = pd.Categorical(df['ground_state'], ground_states)
        df['æ€§'] = pd.Categorical(df['æ€§'], sexes)
        df = pd.get_dummies(df, columns=['weather', 'race_type', 'ground_state', 'æ€§'])

        self.data_c = df

class Results(DataProcessor):
    def __init__(self, results):
        super(Results, self).__init__()
        self.data = results

    @classmethod
    def read_pickle(cls, path_list):
        df = pd.read_pickle(path_list[0])
        for path in path_list[1:]:
            df = update_data(df, pd.read_pickle(path))
        return cls(df)

    @staticmethod
    def scrape(race_id_list):
        """
        ãƒ¬ãƒ¼ã‚¹çµæœãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã™ã‚‹é–¢æ•°

        Parameters:
        ----------
        race_id_list : list
            ãƒ¬ãƒ¼ã‚¹IDã®ãƒªã‚¹ãƒˆ

        Returns:
        ----------
        race_results_df : pandas.DataFrame
            å…¨ãƒ¬ãƒ¼ã‚¹çµæœãƒ‡ãƒ¼ã‚¿ã‚’ã¾ã¨ã‚ã¦DataFrameå‹ã«ã—ãŸã‚‚ã®
        """

        #race_idã‚’keyã«ã—ã¦DataFrameå‹ã‚’æ ¼ç´
        race_results = {}
        for race_id in (race_id_list):
            time.sleep(1)
            try:
                url = "https://db.netkeiba.com/race/" + race_id

                html = requests.get(url)
                html.encoding = "EUC-JP"

                #ãƒ¡ã‚¤ãƒ³ã¨ãªã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                buffer = StringIO(html.text)
                df = pd.read_html(buffer)[0]
                # åˆ—åã«åŠè§’ã‚¹ãƒšãƒ¼ã‚¹ãŒã‚ã‚Œã°é™¤å»ã™ã‚‹
                df = df.rename(columns=lambda x: x.replace(' ', ''))

                # å¤©å€™ã€ãƒ¬ãƒ¼ã‚¹ã®ç¨®é¡ã€ã‚³ãƒ¼ã‚¹ã®é•·ã•ã€é¦¬å ´ã®çŠ¶æ…‹ã€æ—¥ä»˜ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
                soup = BeautifulSoup(html.text, "html.parser")
                #å¤©å€™ã€ãƒ¬ãƒ¼ã‚¹ã®ç¨®é¡ã€ã‚³ãƒ¼ã‚¹ã®é•·ã•ã€é¦¬å ´ã®çŠ¶æ…‹ã€æ—¥ä»˜ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
                texts = (
                    soup.find("div", attrs={"class": "data_intro"}).find_all("p")[0].text
                    + soup.find("div", attrs={"class": "data_intro"}).find_all("p")[1].text
                )
                info = re.findall(r'\w+', texts)
                for text in info:
                    if text in ["èŠ", "ãƒ€ãƒ¼ãƒˆ"]:
                        df["race_type"] = [text] * len(df)
                    if "éšœ" in text:
                        df["race_type"] = ["éšœå®³"] * len(df)
                    if "m" in text:
                        df["course_len"] = [int(re.findall(r"\d+", text)[-1])] * len(df) #20211212ï¼š[0]â†’[-1]ã«ä¿®æ­£
                    if text in ["è‰¯", "ç¨é‡", "é‡", "ä¸è‰¯"]:
                        df["ground_state"] = [text] * len(df)
                    if text in ["æ›‡", "æ™´", "é›¨", "å°é›¨", "å°é›ª", "é›ª"]:
                        df["weather"] = [text] * len(df)
                    if "å¹´" in text:
                        df["date"] = [text] * len(df)

                #é¦¬IDã€é¨æ‰‹IDã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
                horse_id_list = []
                horse_a_list = soup.find("table", attrs={"summary": "ãƒ¬ãƒ¼ã‚¹çµæœ"}).find_all(
                    "a", attrs={"href": re.compile("^/horse")}
                )
                for a in horse_a_list:
                    horse_id = re.findall(r"\d+", a["href"])
                    horse_id_list.append(horse_id[0])
                jockey_id_list = []
                jockey_a_list = soup.find("table", attrs={"summary": "ãƒ¬ãƒ¼ã‚¹çµæœ"}).find_all(
                    "a", attrs={"href": re.compile("^/jockey")}
                )
                for a in jockey_a_list:
                    jockey_id = re.findall(r"\d+", a["href"])
                    jockey_id_list.append(jockey_id[0])
                df["horse_id"] = horse_id_list
                df["jockey_id"] = jockey_id_list

                #ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’race_idã«ã™ã‚‹
                df.index = [race_id] * len(df)

                race_results[race_id] = df
            #å­˜åœ¨ã—ãªã„race_idã‚’é£›ã°ã™
            except IndexError:
                continue
            except AttributeError: #å­˜åœ¨ã—ãªã„race_idã§AttributeErrorã«ãªã‚‹ãƒšãƒ¼ã‚¸ã‚‚ã‚ã‚‹ã®ã§è¿½åŠ 
                continue
            #wifiã®æ¥ç¶šãŒåˆ‡ã‚ŒãŸæ™‚ãªã©ã§ã‚‚é€”ä¸­ã¾ã§ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã›ã‚‹ã‚ˆã†ã«ã™ã‚‹
            except Exception as e:
                print(e)
                break
            #Jupyterã§åœæ­¢ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ãŸæ™‚ã®å¯¾å‡¦
            except:
                break

        #pd.DataFrameå‹ã«ã—ã¦ä¸€ã¤ã®ãƒ‡ãƒ¼ã‚¿ã«ã¾ã¨ã‚ã‚‹
        race_results_df = pd.concat([race_results[key] for key in race_results])
        race_results_df.columns = race_results_df.columns.str.replace(' ', '')
        # åˆ—åã«åŠè§’ã‚¹ãƒšãƒ¼ã‚¹ãŒã‚ã‚Œã°é™¤å»ã™ã‚‹ï¼ˆå…¨ä½“ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ï¼‰
        race_results_df = race_results_df.rename(columns=lambda x: x.replace(' ', ''))
        return race_results_df

    #å‰å‡¦ç†
    def preprocessing(self):
        df = self.data.copy()

        # ç€é †ã«æ•°å­—ä»¥å¤–ã®æ–‡å­—åˆ—ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‚‚ã®ã‚’å–ã‚Šé™¤ã
        # å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ã‚’å‰Šé™¤
        #df['ç€é †'] = pd.to_numeric(df['ç€é †'], errors='coerce')
        #df.dropna(subset=['ç€é †'], inplace=True)
        #df['ç€é †'] = df['ç€é †'].astype(int)
        #df['rank'] = df['ç€é †'].map(lambda x:1 if x<4 else 0)

        # æ€§é½¢ã‚’æ€§ã¨å¹´é½¢ã«åˆ†ã‘ã‚‹
        df["æ€§"] = df["æ€§é½¢"].map(lambda x: str(x)[0])
        df["å¹´é½¢"] = df["æ€§é½¢"].map(lambda x: str(x)[1:]).astype(int)

        # æƒ…å ±é–‹ç¤ºå‰ã®å ´åˆ
        df["é¦¬ä½“é‡(å¢—æ¸›)"].fillna('0(0)', inplace=True)

        # é¦¬ä½“é‡ã‚’ä½“é‡ã¨ä½“é‡å¤‰åŒ–ã«åˆ†ã‘ã‚‹
        df = df[df["é¦¬ä½“é‡(å¢—æ¸›)"] != '--']
        df["ä½“é‡"] = df["é¦¬ä½“é‡(å¢—æ¸›)"].str.split("(", expand=True)[0].astype(int)
        df["ä½“é‡å¤‰åŒ–"] = df["é¦¬ä½“é‡(å¢—æ¸›)"].str.split("(", expand=True)[1].str[:-1]
        # 2020/12/13è¿½åŠ ï¼šå¢—æ¸›ãŒã€Œå‰è¨ˆä¸ã€ãªã©ã®ã¨ãæ¬ æå€¤ã«ã™ã‚‹
        df['ä½“é‡å¤‰åŒ–'] = pd.to_numeric(df['ä½“é‡å¤‰åŒ–'], errors='coerce')

        df["date"] = pd.to_datetime(df["date"])


        # å˜å‹ã‚’floatã«å¤‰æ›
        #df["å˜å‹"] = df["å˜å‹"].astype(float)
        # è·é›¢ã¯10ã®ä½ã‚’åˆ‡ã‚Šæ¨ã¦ã‚‹
        df["course_len"] = df["course_len"].astype(float) // 100

        # ä¸è¦ãªåˆ—ã‚’å‰Šé™¤
        df.drop(["æ€§é½¢", 'é¦¬å', 'é¨æ‰‹', 'äººæ°—'],
                axis=1, inplace=True)

        df["date"] = pd.to_datetime(df["date"], format="%Yå¹´%mæœˆ%dæ—¥")

        #é–‹å‚¬å ´æ‰€
        df['é–‹å‚¬'] = df.index.map(lambda x:str(x)[4:6])

        #6/6å‡ºèµ°æ•°è¿½åŠ 
        df['n_horses'] = df.index.map(df.index.value_counts())

        self.data_p = df

    #ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®å‡¦ç†
    def process_categorical(self):
        self.le_horse = LabelEncoder().fit(self.data_pe['horse_id'])
        self.le_jockey = LabelEncoder().fit(self.data_pe['jockey_id'])
        super().process_categorical(self.le_horse, self.le_jockey, self.data_pe)

class ShutubaTable(DataProcessor):
    def __init__(self, shutuba_tables):
        super(ShutubaTable, self).__init__()
        self.data = shutuba_tables

    @classmethod
    def scrape(cls, race_id_list, date):
        data = pd.DataFrame()
        for race_id in (race_id_list):
            time.sleep(1)
            url = 'https://race.netkeiba.com/race/shutuba.html?race_id=' + race_id

            html = requests.get(url)
            html.encoding = "EUC-JP"

            buffer = StringIO(html.text)
            df = pd.read_html(buffer)[0]

            # åˆ—åã«åŠè§’ã‚¹ãƒšãƒ¼ã‚¹ãŒã‚ã‚Œã°é™¤å»ã™ã‚‹
            df = df.rename(columns=lambda x: x.replace(' ', ''))
            df = df.T.reset_index(level=0, drop=True).T

            soup = BeautifulSoup(html.text, "html.parser")

            texts = soup.find('div', attrs={'class': 'RaceData01'}).text
            texts = re.findall(r'\w+', texts)
            for text in texts:
                if 'm' in text:
                    df['course_len'] = [int(re.findall(r'\d+', text)[-1])] * len(df) #20211212ï¼š[0]â†’[-1]ã«ä¿®æ­£
                if text in ["æ›‡", "æ™´", "é›¨", "å°é›¨", "å°é›ª", "é›ª"]:
                    df["weather"] = [text] * len(df)
                if text in ["è‰¯", "ç¨é‡", "é‡"]:
                    df["ground_state"] = [text] * len(df)
                if 'ä¸' in text:
                    df["ground_state"] = ['ä¸è‰¯'] * len(df)
                # 2020/12/13è¿½åŠ 
                if 'ç¨' in text:
                    df["ground_state"] = ['ç¨é‡'] * len(df)
                if 'èŠ' in text:
                    df['race_type'] = ['èŠ'] * len(df)
                if 'éšœ' in text:
                    df['race_type'] = ['éšœå®³'] * len(df)
                if 'ãƒ€' in text:
                    df['race_type'] = ['ãƒ€ãƒ¼ãƒˆ'] * len(df)
            df['date'] = [date] * len(df)

            # horse_id
            horse_id_list = []
            horse_td_list = soup.find_all("td", attrs={'class': 'HorseInfo'})
            for td in horse_td_list:
                horse_id = re.findall(r'\d+', td.find('a')['href'])[0]
                horse_id_list.append(horse_id)
            # jockey_id
            jockey_id_list = []
            jockey_td_list = soup.find_all("td", attrs={'class': 'Jockey'})
            for td in jockey_td_list:
                jockey_id = re.findall(r'\d+', td.find('a')['href'])[0]
                jockey_id_list.append(jockey_id)
            df['horse_id'] = horse_id_list
            df['jockey_id'] = jockey_id_list

            df.index = [race_id] * len(df)
            data = pd.concat([data, df])
        return cls(data)

    #å‰å‡¦ç†
    def preprocessing(self):
        df = self.data.copy()

        df["æ€§"] = df["æ€§é½¢"].map(lambda x: str(x)[0])
        df["å¹´é½¢"] = df["æ€§é½¢"].map(lambda x: str(x)[1:]).astype(int)

        # æƒ…å ±é–‹ç¤ºå‰ã®å ´åˆ
        df["é¦¬ä½“é‡(å¢—æ¸›)"].fillna('0(0)', inplace=True)

        # é¦¬ä½“é‡ã‚’ä½“é‡ã¨ä½“é‡å¤‰åŒ–ã«åˆ†ã‘ã‚‹
        df = df[df["é¦¬ä½“é‡(å¢—æ¸›)"] != '--']
        df["ä½“é‡"] = df["é¦¬ä½“é‡(å¢—æ¸›)"].str.split("(", expand=True)[0].astype(int)
        df["ä½“é‡å¤‰åŒ–"] = df["é¦¬ä½“é‡(å¢—æ¸›)"].str.split("(", expand=True)[1].str[:-1]
        # 2020/12/13è¿½åŠ ï¼šå¢—æ¸›ãŒã€Œå‰è¨ˆä¸ã€ãªã©ã®ã¨ãæ¬ æå€¤ã«ã™ã‚‹
        df['ä½“é‡å¤‰åŒ–'] = pd.to_numeric(df['ä½“é‡å¤‰åŒ–'], errors='coerce')

        df["date"] = pd.to_datetime(df["date"])

        df['æ '] = df['æ '].astype(int)
        df['é¦¬ç•ª'] = df['é¦¬ç•ª'].astype(int)
        df['æ–¤é‡'] = df['æ–¤é‡'].astype(int)
        df['é–‹å‚¬'] = df.index.map(lambda x:str(x)[4:6])

        #6/6å‡ºèµ°æ•°è¿½åŠ 
        df['n_horses'] = df.index.map(df.index.value_counts())

        # è·é›¢ã¯10ã®ä½ã‚’åˆ‡ã‚Šæ¨ã¦ã‚‹
        df["course_len"] = df["course_len"].astype(float) // 100

        # ä½¿ç”¨ã™ã‚‹åˆ—ã‚’é¸æŠ
        df = df[['æ ', 'é¦¬ç•ª', 'æ–¤é‡', 'course_len', 'weather','race_type',
        'ground_state', 'date', 'horse_id', 'jockey_id', 'æ€§', 'å¹´é½¢',
       'ä½“é‡', 'ä½“é‡å¤‰åŒ–', 'é–‹å‚¬', 'n_horses']]

        self.data_p = df.rename(columns={'æ ': 'æ ç•ª'})

class HorseResults:
    def __init__(self, horse_results):
        self.horse_results = horse_results[['æ—¥ä»˜', 'ç€é †', 'è³é‡‘', 'ç€å·®', 'é€šé', 'é–‹å‚¬', 'è·é›¢']]
        self.preprocessing()

    @classmethod
    def read_pickle(cls, path_list):
        df = pd.read_pickle(path_list[0])
        for path in path_list[1:]:
            df = update_data(df, pd.read_pickle(path))
        return cls(df)




    @staticmethod
    def scrape(horse_id_list):
        horse_results = {}
        for horse_id in (horse_id_list):
            time.sleep(1)
            try:
                url = 'https://db.netkeiba.com/horse/' + horse_id
                res = requests.get(url)
                res.encoding = 'EUC-JP'  # ã¾ãŸã¯ã‚µã‚¤ãƒˆã«åˆã‚ã›ãŸã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
                df = pd.read_html(res.text)[3]
            
                if df.columns[0]=='å—è³æ­´':
                    df = pd.read_html(url)[4]
                df.index = [horse_id] * len(df)
                horse_results[horse_id] = df
            except IndexError:
                st.error(f"IndexError occurred for horse_id: {horse_id}")
                continue
            except Exception as e:
                st.error(f"An exception occurred: {e}")
                break

        if not horse_results:
            st.error("horse_results is empty")
            return

        #pd.DataFrameå‹ã«ã—ã¦ä¸€ã¤ã®ãƒ‡ãƒ¼ã‚¿ã«ã¾ã¨ã‚ã‚‹
        horse_results_df = pd.concat([horse_results[key] for key in horse_results])

        return horse_results_df

    def preprocessing(self):
        df = self.horse_results.copy()

        # ç€é †ã«æ•°å­—ä»¥å¤–ã®æ–‡å­—åˆ—ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‚‚ã®ã‚’å–ã‚Šé™¤ã
        df['ç€é †'] = pd.to_numeric(df['ç€é †'], errors='coerce')
        df.dropna(subset=['ç€é †'], inplace=True)
        df['ç€é †'] = df['ç€é †'].astype(int)

        df["date"] = pd.to_datetime(df["æ—¥ä»˜"])
        df.drop(['æ—¥ä»˜'], axis=1, inplace=True)

        #è³é‡‘ã®NaNã‚’0ã§åŸ‹ã‚ã‚‹
        df['è³é‡‘'].fillna(0, inplace=True)

        #1ç€ã®ç€å·®ã‚’0ã«ã™ã‚‹
        df['ç€å·®'] = df['ç€å·®'].map(lambda x: 0 if x<0 else x)

        #ãƒ¬ãƒ¼ã‚¹å±•é–‹ãƒ‡ãƒ¼ã‚¿
        #n=1: æœ€åˆã®ã‚³ãƒ¼ãƒŠãƒ¼ä½ç½®, n=4: æœ€çµ‚ã‚³ãƒ¼ãƒŠãƒ¼ä½ç½®
        def corner(x, n):
            if type(x) != str:
                return x
            elif n==4:
                return int(re.findall(r'\d+', x)[-1])
            elif n==1:
                return int(re.findall(r'\d+', x)[0])
        df['first_corner'] = df['é€šé'].map(lambda x: corner(x, 1))
        df['final_corner'] = df['é€šé'].map(lambda x: corner(x, 4))

        df['final_to_rank'] = df['final_corner'] - df['ç€é †']
        df['first_to_rank'] = df['first_corner'] - df['ç€é †']
        df['first_to_final'] = df['first_corner'] - df['final_corner']

        #é–‹å‚¬å ´æ‰€
        df['é–‹å‚¬'] = df['é–‹å‚¬'].str.extract(r'(\D+)')[0].map(place_dict).fillna('11')
        #race_type
        df['race_type'] = df['è·é›¢'].str.extract(r'(\D+)')[0].map(race_type_dict)
        #è·é›¢ã¯10ã®ä½ã‚’åˆ‡ã‚Šæ¨ã¦ã‚‹
        #ä¸€éƒ¨ã®é¦¬ã§æ¬ æå€¤ãŒã‚ã‚Šã€intã«å¤‰æ›ã§ããªã„ãŸã‚floatã«å¤‰æ›ã™ã‚‹
        df['course_len'] = df['è·é›¢'].str.extract(r'(\d+)').astype(float) // 100
        df.drop(['è·é›¢'], axis=1, inplace=True)
        #ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åã‚’ä¸ãˆã‚‹
        df.index.name = 'horse_id'

        self.horse_results = df
        self.target_list = ['ç€é †', 'è³é‡‘', 'ç€å·®', 'first_corner', 'final_corner',
                            'first_to_rank', 'first_to_final','final_to_rank']

    #n_samplesãƒ¬ãƒ¼ã‚¹åˆ†é¦¬ã”ã¨ã«å¹³å‡ã™ã‚‹
    def average(self, horse_id_list, date, n_samples='all'):
        target_df = self.horse_results.query('index in @horse_id_list')

        #éå»ä½•èµ°åˆ†å–ã‚Šå‡ºã™ã‹æŒ‡å®š
        if n_samples == 'all':
            filtered_df = target_df[target_df['date'] < date]
        elif n_samples > 0:
            filtered_df = target_df[target_df['date'] < date].\
                sort_values('date', ascending=False).groupby(level=0).head(n_samples)
        else:
            raise Exception('n_samples must be >0')

        #é›†è¨ˆã—ã¦è¾æ›¸å‹ã«å…¥ã‚Œã‚‹
        self.average_dict = {}
        self.average_dict['non_category'] = filtered_df.groupby(level=0)[self.target_list].mean()\
            .add_suffix('_{}R'.format(n_samples))
        for column in ['course_len', 'race_type', 'é–‹å‚¬']:
            self.average_dict[column] = filtered_df.groupby(['horse_id', column])\
                [self.target_list].mean().add_suffix('_{}_{}R'.format(column, n_samples))

        #6/6è¿½åŠ : é¦¬ã®å‡ºèµ°é–“éš”è¿½åŠ ã®ãŸã‚ã«ã€å…¨ãƒ¬ãƒ¼ã‚¹ã®æ—¥ä»˜ã‚’å¤‰æ•°latestã«æ ¼ç´
        if n_samples == 5:
            self.latest = filtered_df.groupby('horse_id')['date'].max().rename('latest')

    def merge(self, results, date, n_samples='all'):
        df = results[results['date']==date]
        horse_id_list = df['horse_id']
        self.average(horse_id_list, date, n_samples)
        merged_df = df.merge(self.average_dict['non_category'], left_on='horse_id',
                             right_index=True, how='left')
        for column in ['course_len','race_type', 'é–‹å‚¬']:
            merged_df = merged_df.merge(self.average_dict[column],
                                        left_on=['horse_id', column],
                                        right_index=True, how='left')

        #6/6è¿½åŠ ï¼šé¦¬ã®å‡ºèµ°é–“éš”è¿½åŠ ã®ãŸã‚ã«ã€å…¨ãƒ¬ãƒ¼ã‚¹ã®æ—¥ä»˜ã‚’å¤‰æ•°latestã«æ ¼ç´
        if n_samples == 5:
            merged_df = merged_df.merge(self.latest, left_on='horse_id',
                             right_index=True, how='left')
        return merged_df

    def merge_all(self, results, n_samples='all'):
        date_list = results['date'].unique()
        merged_df = pd.concat([self.merge(results, date, n_samples) for date in (date_list)])
        return merged_df

class Peds:
    def __init__(self, peds):
        self.peds = peds
        self.peds_e = pd.DataFrame() #after label encoding and transforming into category

    @classmethod
    def read_pickle(cls, path_list):
        df = pd.read_pickle(path_list[0])
        for path in path_list[1:]:
            df = update_data(df, pd.read_pickle(path))
        return cls(df)


    @staticmethod
    def scrape(horse_id_list):
        peds_dict = {}

        # horse_id_listãŒç©ºã§ãªã„ã‹ç¢ºèª
        if len(horse_id_list) == 0:
            print("No horse IDs to scrape.")
            return pd.DataFrame()

        for horse_id in (horse_id_list):
            time.sleep(1)
            try:
                url = "https://db.netkeiba.com/horse/ped/" + horse_id
                html = requests.get(url)
                html.encoding = "EUC-JP"
                df = pd.read_html(html.text)[0]

                generations = {}
                for i in reversed(range(5)):
                    generations[i] = df[i]
                    df.drop([i], axis=1, inplace=True)
                    df = df.drop_duplicates()
                ped = pd.concat([generations[i] for i in range(5)]).rename(horse_id)
                peds_dict[horse_id] = ped.reset_index(drop=True)

            except IndexError:
                continue
            except Exception as e:
                print(e)
                break
            except:
                break

        if len(peds_dict) == 0:
            print("No data to concatenate. Skipping...")
            return pd.DataFrame()

        peds_df = pd.concat([peds_dict[key] for key in peds_dict], axis=1).T.add_prefix('peds_')

        return peds_df



    def encode(self):
        df = self.peds.copy()
        for column in df.columns:
            df[column] = LabelEncoder().fit_transform(df[column].fillna('Na'))
        self.peds_e = df.astype('category')

        
#é–‹å‚¬å ´æ‰€ã‚’idã«å¤‰æ›ã™ã‚‹ãŸã‚ã®è¾æ›¸å‹
place_dict = {
    'æœ­å¹Œ':'01',  'å‡½é¤¨':'02',  'ç¦å³¶':'03',  'æ–°æ½Ÿ':'04',  'æ±äº¬':'05',
    'ä¸­å±±':'06',  'ä¸­äº¬':'07',  'äº¬éƒ½':'08',  'é˜ªç¥':'09',  'å°å€‰':'10'
}

#ãƒ¬ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ã‚’ãƒ¬ãƒ¼ã‚¹çµæœãƒ‡ãƒ¼ã‚¿ã¨æ•´åˆã•ã›ã‚‹ãŸã‚ã®è¾æ›¸å‹
race_type_dict = {
    'èŠ': 'èŠ', 'ãƒ€': 'ãƒ€ãƒ¼ãƒˆ', 'éšœ': 'éšœå®³'
}



# ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹é–¢æ•°
def update_data(old, new):
    """
    Parameters:
    ----------
    old : pandas.DataFrame
        å¤ã„ãƒ‡ãƒ¼ã‚¿
    new : pandas.DataFrame
        æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿
    """

    filtered_old = old[~old.index.isin(new.index)]
    return pd.concat([filtered_old, new])

def load_data(base_race_id):
    url = f"https://race.netkeiba.com/race/shutuba.html?race_id={base_race_id}"
    html = requests.get(url)
    html.encoding = "EUC-JP"

    try:
        html_text = html.text
        buffer = StringIO(html_text)
        df = pd.read_html(buffer)[0]
        df.columns = df.iloc[0]
        column_names = ['æ ', 'é¦¬ ç•ª', 'å°', 'é¦¬å', 'æ€§é½¢', 'æ–¤é‡', 'é¨æ‰‹', 'å©èˆ', 'é¦¬ä½“é‡ (å¢—æ¸›)', 'äºˆæƒ³ã‚ªãƒƒã‚º', 'äººæ°—', 'ç™»éŒ²', 'ãƒ¡ãƒ¢']
        df.columns = column_names
        df.drop(['å°', 'ç™»éŒ²', 'ãƒ¡ãƒ¢', 'äºˆæƒ³ã‚ªãƒƒã‚º', 'äººæ°—'], axis=1, inplace=True)
        return df
    except Exception as e:
        st.write(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return None

def load_additional_data(base_race_id):
    url = f"https://race.netkeiba.com/race/shutuba.html?race_id={base_race_id}"
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')

    try:
        race_name_div = soup.find('div', class_='RaceName')
        race_name = race_name_div.text.strip()

        # Icon_GradeType13 ã‚¯ãƒ©ã‚¹ã‚’æŒã¤ span ã‚¿ã‚°ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
        if race_name_div.find('span', class_='Icon_GradeType13'):
            race_name += '_win5'

        race_data01 = soup.find('div', class_='RaceData01').text.strip()
        race_data02 = soup.find('div', class_='RaceData02').text.strip().replace('\xa0', ' ')

        return {
            'race_name': race_name,
            'race_data01': race_data01,
            'race_data02': race_data02
        }
    except Exception as e:
        st.write(f"è¿½åŠ ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return None

def generate_column_names():
    column_names = ['æ ç•ª',
 'é¦¬ç•ª',
 'æ–¤é‡',
 'course_len',
 'horse_id',
 'jockey_id',
 'å¹´é½¢',
 'ä½“é‡',
 'ä½“é‡å¤‰åŒ–',
 'n_horses',
 'ç€é †_5R',
 'è³é‡‘_5R',
 'ç€å·®_5R',
 'first_corner_5R',
 'final_corner_5R',
 'first_to_rank_5R',
 'first_to_final_5R',
 'final_to_rank_5R',
 'ç€é †_course_len_5R',
 'è³é‡‘_course_len_5R',
 'ç€å·®_course_len_5R',
 'first_corner_course_len_5R',
 'final_corner_course_len_5R',
 'first_to_rank_course_len_5R',
 'first_to_final_course_len_5R',
 'final_to_rank_course_len_5R',
 'ç€é †_race_type_5R',
 'è³é‡‘_race_type_5R',
 'ç€å·®_race_type_5R',
 'first_corner_race_type_5R',
 'final_corner_race_type_5R',
 'first_to_rank_race_type_5R',
 'first_to_final_race_type_5R',
 'final_to_rank_race_type_5R',
 'ç€é †_é–‹å‚¬_5R',
 'è³é‡‘_é–‹å‚¬_5R',
 'ç€å·®_é–‹å‚¬_5R',
 'first_corner_é–‹å‚¬_5R',
 'final_corner_é–‹å‚¬_5R',
 'first_to_rank_é–‹å‚¬_5R',
 'first_to_final_é–‹å‚¬_5R',
 'final_to_rank_é–‹å‚¬_5R',
 'ç€é †_9R',
 'è³é‡‘_9R',
 'ç€å·®_9R',
 'first_corner_9R',
 'final_corner_9R',
 'first_to_rank_9R',
 'first_to_final_9R',
 'final_to_rank_9R',
 'ç€é †_course_len_9R',
 'è³é‡‘_course_len_9R',
 'ç€å·®_course_len_9R',
 'first_corner_course_len_9R',
 'final_corner_course_len_9R',
 'first_to_rank_course_len_9R',
 'first_to_final_course_len_9R',
 'final_to_rank_course_len_9R',
 'ç€é †_race_type_9R',
 'è³é‡‘_race_type_9R',
 'ç€å·®_race_type_9R',
 'first_corner_race_type_9R',
 'final_corner_race_type_9R',
 'first_to_rank_race_type_9R',
 'first_to_final_race_type_9R',
 'final_to_rank_race_type_9R',
 'ç€é †_é–‹å‚¬_9R',
 'è³é‡‘_é–‹å‚¬_9R',
 'ç€å·®_é–‹å‚¬_9R',
 'first_corner_é–‹å‚¬_9R',
 'final_corner_é–‹å‚¬_9R',
 'first_to_rank_é–‹å‚¬_9R',
 'first_to_final_é–‹å‚¬_9R',
 'final_to_rank_é–‹å‚¬_9R',
 'ç€é †_allR',
 'è³é‡‘_allR',
 'ç€å·®_allR',
 'first_corner_allR',
 'final_corner_allR',
 'first_to_rank_allR',
 'first_to_final_allR',
 'final_to_rank_allR',
 'ç€é †_course_len_allR',
 'è³é‡‘_course_len_allR',
 'ç€å·®_course_len_allR',
 'first_corner_course_len_allR',
 'final_corner_course_len_allR',
 'first_to_rank_course_len_allR',
 'first_to_final_course_len_allR',
 'final_to_rank_course_len_allR',
 'ç€é †_race_type_allR',
 'è³é‡‘_race_type_allR',
 'ç€å·®_race_type_allR',
 'first_corner_race_type_allR',
 'final_corner_race_type_allR',
 'first_to_rank_race_type_allR',
 'first_to_final_race_type_allR',
 'final_to_rank_race_type_allR',
 'ç€é †_é–‹å‚¬_allR',
 'è³é‡‘_é–‹å‚¬_allR',
 'ç€å·®_é–‹å‚¬_allR',
 'first_corner_é–‹å‚¬_allR',
 'final_corner_é–‹å‚¬_allR',
 'first_to_rank_é–‹å‚¬_allR',
 'first_to_final_é–‹å‚¬_allR',
 'final_to_rank_é–‹å‚¬_allR',
 'interval',
 'peds_0',
 'peds_1',
 'peds_2',
 'peds_3',
 'peds_4',
 'peds_5',
 'peds_6',
 'peds_7',
 'peds_8',
 'peds_9',
 'peds_10',
 'peds_11',
 'peds_12',
 'peds_13',
 'peds_14',
 'peds_15',
 'peds_16',
 'peds_17',
 'peds_18',
 'peds_19',
 'peds_20',
 'peds_21',
 'peds_22',
 'peds_23',
 'peds_24',
 'peds_25',
 'peds_26',
 'peds_27',
 'peds_28',
 'peds_29',
 'peds_30',
 'peds_31',
 'peds_32',
 'peds_33',
 'peds_34',
 'peds_35',
 'peds_36',
 'peds_37',
 'peds_38',
 'peds_39',
 'peds_40',
 'peds_41',
 'peds_42',
 'peds_43',
 'peds_44',
 'peds_45',
 'peds_46',
 'peds_47',
 'peds_48',
 'peds_49',
 'peds_50',
 'peds_51',
 'peds_52',
 'peds_53',
 'peds_54',
 'peds_55',
 'peds_56',
 'peds_57',
 'peds_58',
 'peds_59',
 'peds_60',
 'peds_61',
 'weather_æ™´',
 'weather_æ›‡',
 'weather_é›¨',
 'weather_å°é›¨',
 'weather_å°é›ª',
 'weather_é›ª',
 'race_type_èŠ',
 'race_type_ãƒ€ãƒ¼ãƒˆ',
 'race_type_éšœå®³',
 'ground_state_è‰¯',
 'ground_state_ç¨é‡',
 'ground_state_é‡',
 'ground_state_ä¸è‰¯',
 'æ€§_ç‰',
 'æ€§_ç‰¡',
 'æ€§_ã‚»']
    return column_names


# Streamlit UI
st.title("ç«¶é¦¬AIäºˆæƒ³ğŸ")

# ç¾åœ¨ã®æ—¥ä»˜ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¨ã—ã¦è¨­å®š
today = datetime.date.today()
# date_input ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆã§æ—¥ä»˜ã‚’é¸æŠ
selected_date = st.date_input("é–‹å‚¬æ—¥ã‚’é¸æŠã—ã¦ãã ã•ã„", today)
# é¸æŠã•ã‚ŒãŸæ—¥ä»˜ã‚’ YYYY/MM/DD å½¢å¼ã§è¡¨ç¤º
formatted_date = selected_date.strftime("%Y/%m/%d")

racecourse_map = {
    "æœ­å¹Œ_01": "01",
    "å‡½é¤¨_02": "02",
    "ç¦å³¶_03": "03",
    "æ–°æ½Ÿ_04": "04",
    "æ±äº¬_05": "05",
    "ä¸­å±±_06": "06",
    "ä¸­äº¬_07": "07",
    "äº¬éƒ½_08": "08",
    "é˜ªç¥_09": "09",
    "å°å€‰_10": "10"
}

racecourse = st.selectbox("ç«¶é¦¬å ´ã‚’é¸æŠã—ã¦ãã ã•ã„", list(racecourse_map.keys()))
holding_number = st.selectbox("é–‹å‚¬å›æ•°ã‚’é¸æŠã—ã¦ãã ã•ã„", list(range(1, 12)))

day_options = ["1æ—¥ç›®", "2æ—¥ç›®", "3æ—¥ç›®", "4æ—¥ç›®", "5æ—¥ç›®", "6æ—¥ç›®", "7æ—¥ç›®", "8æ—¥ç›®", "9æ—¥ç›®", "10æ—¥ç›®"]
selected_day = st.selectbox("ä½•æ—¥ç›®ã‹é¸æŠã—ã¦ãã ã•ã„", day_options)
day_number = int(selected_day[0])

race_number = st.selectbox("ä½•ãƒ¬ãƒ¼ã‚¹ã‹ã‚’é¸æŠã—ã¦ãã ã•ã„", list(range(1, 12)))



base_race_id = f"2023{racecourse_map[racecourse]}{holding_number:02d}{day_number:02d}{race_number:02d}"


st.write(f"é¸æŠã•ã‚ŒãŸæ—¥ä»˜ã¯ {formatted_date} ã§ã™")
st.write(f"RACE_IDã¯ {base_race_id} ã§ã™ã€‚")


# ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰
df = load_data(base_race_id)
additional_data = load_additional_data(base_race_id)

if additional_data:
    st.write(f"ãƒ¬ãƒ¼ã‚¹å: {additional_data['race_name']}")


# DataFrameã‚’è¡¨ç¤º
if df is not None:
    st.table(df)
else:
    st.write('ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚')






if st.button('AIäºˆæƒ³'):
    st.write('AIäºˆæƒ³ã‚’é–‹å§‹è‡´ã—ã¾ã™ã€‚å‡¦ç†ã«ã¯15åˆ†ã€œ20åˆ†ã‹ã‹ã‚Šã¾ã™ã€‚')

    #race_id_list ã®ç”Ÿæˆ
    #race_id_list = [f"{2023010101}{str(i).zfill(2)}" for i in range(1, 13)]
    race_id_list = [base_race_id]
    sta = ShutubaTable.scrape(race_id_list, formatted_date)
    sta.data = sta.data.rename(columns=lambda x: x.replace(' ', ''))
    horse_id_list = sta.data['horse_id'].unique()
    #å‰å‡¦ç†
    sta.preprocessing()
    st.write("å‡ºé¦¬è¡¨: ", sta.data)
    
    horse_results = HorseResults.scrape(horse_id_list)
    

    horse_results = horse_results.rename(columns=lambda x: x.replace(' ', ''))
    st.write("å‡ºèµ°é¦¬ã®éå»æˆç¸¾æƒ…å ±: ", horse_results)
    #é¦¬ã®éå»æˆç¸¾ãƒ‡ãƒ¼ã‚¿è¿½åŠ 
    hr = HorseResults(horse_results)
    #é¦¬ã®éå»æˆç¸¾ãƒ‡ãƒ¼ã‚¿ã®è¿½åŠ ã€‚æ–°é¦¬ã¯NaNãŒè¿½åŠ ã•ã‚Œã‚‹
    sta.merge_horse_results(hr)



    peds = Peds.scrape(horse_id_list)
    st.write("å‡ºèµ°é¦¬ã®è¡€çµ±æƒ…å ±: ", peds)

    p = Peds(peds)
    p.encode()

    sta.merge_horse_results(hr, n_samples_list=[5, 9, 'all'])

    #5ä¸–ä»£åˆ†ã®è¡€çµ±ãƒ‡ãƒ¼ã‚¿ã®è¿½åŠ 
    sta.merge_peds(p.peds_e)

    data_pe = sta.data_pe

    # 1. LabelEncoderã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’åˆæœŸåŒ–
    le_horse = LabelEncoder().fit(data_pe['horse_id'])
    le_jockey = LabelEncoder().fit(data_pe['jockey_id'])

    # 2. ãƒ©ãƒ™ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    data_pe['horse_id'] = le_horse.transform(data_pe['horse_id'])
    data_pe['jockey_id'] = le_jockey.transform(data_pe['jockey_id'])

    # 3. pandasã®categoryå‹ã«å¤‰æ›
    data_pe['horse_id'] = data_pe['horse_id'].astype('category')
    data_pe['jockey_id'] = data_pe['jockey_id'].astype('category')

    # 4. ãƒ€ãƒŸãƒ¼å¤‰æ•°åŒ–
    weathers = data_pe['weather'].unique()
    race_types = data_pe['race_type'].unique()
    ground_states = data_pe['ground_state'].unique()
    sexes = data_pe['æ€§'].unique()

    data_pe['weather'] = pd.Categorical(data_pe['weather'], weathers)
    data_pe['race_type'] = pd.Categorical(data_pe['race_type'], race_types)
    data_pe['ground_state'] = pd.Categorical(data_pe['ground_state'], ground_states)
    data_pe['æ€§'] = pd.Categorical(data_pe['æ€§'], sexes)

    data_pe = pd.get_dummies(data_pe, columns=['weather', 'race_type', 'ground_state', 'æ€§'])

    st.write("5ä¸–ä»£åˆ†ã®è¡€çµ±ãƒ‡ãƒ¼ã‚¿ã®è¿½åŠ : ", data_pe)


    # LightGBMãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€
    lgb_clf = lgb.Booster(model_file="lgb_model.txt")

    data_c = data_pe.drop(['date'], axis=1)


    # è¿½åŠ ã—ãŸã„åˆ—åã®ãƒªã‚¹ãƒˆ
    columns_to_add = ['ground_state_ç¨é‡', 'weather_é›ª', 'weather_å°é›¨', 'ground_state_ä¸è‰¯', 'weather_å°é›ª', 'weather_é›¨', 'weather_æ›‡', 'ground_state_é‡', 'race_type_éšœå®³', 'weather_æ™´', 'race_type_èŠ', 'race_type_ãƒ€ãƒ¼ãƒˆ', 'ground_state_è‰¯', 'æ€§_ç‰¡', 'æ€§_ç‰', 'æ€§_ã‚»']
    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å­˜åœ¨ã—ãªã„åˆ—åã ã‘ã‚’è¿½åŠ 
    for col in columns_to_add:
        if col not in data_c.columns:
            data_c[col] = 0  # æ•°å€¤ã®0ã‚’å…¥ã‚Œã‚‹

    my_column_names = generate_column_names()

    # è¨“ç·´æ™‚ã«ä½¿ç”¨ã•ã‚ŒãŸç‰¹å¾´é‡ã®åå‰ã‚’å–å¾—
    train_features = my_column_names

    # äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´é‡ã®åå‰ã‚’å–å¾—
    test_features = data_c.columns.tolist()

    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«ã¯ã‚ã‚‹ãŒã€äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã«ã¯ãªã„ç‰¹å¾´é‡ã‚’è¦‹ã¤ã‘ã‚‹
    missing_features = set(train_features) - set(test_features)

    # ä¸è¶³ã—ã¦ã„ã‚‹ç‰¹å¾´é‡ã«0ã‚’å‰²ã‚Šå½“ã¦ã‚‹
    for feature in missing_features:
        data_c[feature] = 0


    # äºˆæ¸¬ã‚’å®Ÿæ–½
    predictions = lgb_clf.predict(data_c)

    # äºˆæ¸¬çµæœã‚’data_cã«è¿½åŠ 
    data_c['Predicted_Rank'] = predictions

    # äºˆæ¸¬çµæœã‚’é™é †ã«ã‚½ãƒ¼ãƒˆ
    sorted_predictions = data_c.sort_values(by=['Predicted_Rank'], ascending=False)

    # TOP3ã®äºˆæ¸¬çµæœã‚’æŠ½å‡º
    top_3_per_race = data_c.groupby(level=0).apply(lambda x: x.nlargest(3, 'Predicted_Rank'))

    # å„ãƒ¬ãƒ¼ã‚¹ã”ã¨ã«å‡ºé¦¬è¡¨ã¨TOP3ã‚’è¡¨ç¤º
    for race_id, group_data in sorted_predictions.groupby(level=0):

        # dfï¼ˆå‡ºé¦¬è¡¨ï¼‰ã‚’å¯¾å¿œã™ã‚‹race_idã§æ›´æ–°ã™ã‚‹
        df = load_data(race_id)

        if df is not None:
            # å…ˆé ­ã«'AIäºˆæƒ³'åˆ—ã‚’è¿½åŠ ï¼ˆã™ã§ã«å­˜åœ¨ã—ã¦ã„ã‚‹å ´åˆã¯ãã®ã¾ã¾ï¼‰
            if 'AIäºˆæƒ³' not in df.columns:
                df.insert(0, 'AIäºˆæƒ³', None)

            # race_idã¨é¦¬ç•ªãŒä¸€è‡´ã™ã‚‹è¡Œã®'AIäºˆæƒ³'åˆ—ã«Predicted_Rankã®å€¤ã‚’æŒ¿å…¥
            for _, row in group_data.iterrows():
                horse_number = row['é¦¬ç•ª']
                predicted_rank = row['Predicted_Rank']
                df.loc[df['é¦¬ ç•ª'] == horse_number, 'AIäºˆæƒ³'] = predicted_rank

            # è¿½åŠ ã®æƒ…å ±ã¨ã¨ã‚‚ã«å‡ºé¦¬è¡¨ã‚’è¡¨ç¤º
            additional_data = load_additional_data(race_id)
            if additional_data:
                st.markdown(f"**ãƒ¬ãƒ¼ã‚¹å:** {additional_data['race_name']}")

            st.dataframe(df)

        # ãã®ãƒ¬ãƒ¼ã‚¹ã®TOP3äºˆæ¸¬ã‚’è¡¨ç¤º
        if race_id in top_3_per_race.index.levels[0]:
            top_3_data = top_3_per_race.loc[race_id]
            st.dataframe(top_3_data[['é¦¬ç•ª', 'Predicted_Rank']])

